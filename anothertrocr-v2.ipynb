{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10913234,"sourceType":"datasetVersion","datasetId":6783964}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":15425.235942,"end_time":"2024-05-29T17:22:44.950166","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-29T13:05:39.714224","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"03e21ac54cf54a58854e3ff61bd6b5e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"052db6ec33524580b9ac8731f855b180":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f04fb5583ba4dafa019bd78b1a98b6b","placeholder":"​","style":"IPY_MODEL_03e21ac54cf54a58854e3ff61bd6b5e7","value":"preprocessor_config.json: 100%"}},"066b563c1f1a4861b4e26509519fd453":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07f68943022c40158c964063826a74f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98062b1914674e6d8e86f1eab474b8dc","placeholder":"​","style":"IPY_MODEL_bd13ae95969a42c490f9ca3983e1775a","value":"Downloading builder script: "}},"0dce60c3b2b44a08b6b80f3608250028":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_052db6ec33524580b9ac8731f855b180","IPY_MODEL_dd1cdb04a5af40d794da1e6245e20158","IPY_MODEL_7808e5516b5e4ea3860cef54b5f945d7"],"layout":"IPY_MODEL_433eb39c56574acb923d9d96bf359e01"}},"0f04fb5583ba4dafa019bd78b1a98b6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f2ff928bc8d4da6a2c2bcef89337546":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d99a27a34ea94e2ab329c78c9d367b50","placeholder":"​","style":"IPY_MODEL_e54c5da95f1b46a2bb72dee4e07d0e31","value":" 4.17k/4.17k [00:00&lt;00:00, 378kB/s]"}},"1024e749e90f4e548f90c283c561b362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17a4153ab8f44658b72c0f4a39d2995f","IPY_MODEL_810c6dbc094a45f6b8f0c11c559c9fd0","IPY_MODEL_0f2ff928bc8d4da6a2c2bcef89337546"],"layout":"IPY_MODEL_51762989f98344158e060a8afee2c897"}},"17a4153ab8f44658b72c0f4a39d2995f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf1ddda811ab4a3a9210d10a301c3036","placeholder":"​","style":"IPY_MODEL_4bcd0e00bd9e44818e434c3748c7d586","value":"config.json: 100%"}},"294f13e1ca06483d87569f1c4ed4af45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a3f634ddb8458481a7f96ab974cbd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d05c7058574692a1f074a41c47ccb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"433eb39c56574acb923d9d96bf359e01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44d94e813bb24d0e90d97dc89b0eb268":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bcd0e00bd9e44818e434c3748c7d586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d344edbd2944a5ab46a6cf7277bc8db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51762989f98344158e060a8afee2c897":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52eba9574066432191e359fb6f87ec87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a92ec0fd048442fb4cd60ea97191fea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c8b23d76770453197465e64da0048f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7270e75994564a00a7a2e7d0b832e335","placeholder":"​","style":"IPY_MODEL_98ada768021f484a8e7ca8ffc03f3d3a","value":" 5.65k/? [00:00&lt;00:00, 475kB/s]"}},"7270e75994564a00a7a2e7d0b832e335":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77befafbdc97445eb350d77bdd523cf4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7808e5516b5e4ea3860cef54b5f945d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5794ad1cf7a41a1ad884b85f003b793","placeholder":"​","style":"IPY_MODEL_066b563c1f1a4861b4e26509519fd453","value":" 224/224 [00:00&lt;00:00, 18.7kB/s]"}},"782d137cfb1c49ca904b6d9530317d4b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"810c6dbc094a45f6b8f0c11c559c9fd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39d05c7058574692a1f074a41c47ccb5","max":4165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d344edbd2944a5ab46a6cf7277bc8db","value":4165}},"94e5e09f30d2433aaf884dbb00aa8825":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33a3f634ddb8458481a7f96ab974cbd1","placeholder":"​","style":"IPY_MODEL_b6f45a5c0afb4f779602564e8dc0a5b2","value":" 5.59k/? [00:00&lt;00:00, 476kB/s]"}},"98062b1914674e6d8e86f1eab474b8dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98ada768021f484a8e7ca8ffc03f3d3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98c2269ef64c4a4eaa4f602d029dd19b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4d52d3538f54084a8b1cb6ccd8d7c1e","max":2159,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb33b6d155be4c0d81d15576f1af9207","value":2159}},"9fb64d07eb2f4466b8c821cc7fd7230a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_782d137cfb1c49ca904b6d9530317d4b","max":2169,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a92ec0fd048442fb4cd60ea97191fea","value":2169}},"afc6059585684905b36dfcd29c375af7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e14b66dac939446fb2dd804571872df1","IPY_MODEL_98c2269ef64c4a4eaa4f602d029dd19b","IPY_MODEL_94e5e09f30d2433aaf884dbb00aa8825"],"layout":"IPY_MODEL_52eba9574066432191e359fb6f87ec87"}},"b6f45a5c0afb4f779602564e8dc0a5b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd13ae95969a42c490f9ca3983e1775a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be5b2f7538c34958bbf24175eab7566d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07f68943022c40158c964063826a74f0","IPY_MODEL_9fb64d07eb2f4466b8c821cc7fd7230a","IPY_MODEL_6c8b23d76770453197465e64da0048f0"],"layout":"IPY_MODEL_77befafbdc97445eb350d77bdd523cf4"}},"bf1ddda811ab4a3a9210d10a301c3036":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5794ad1cf7a41a1ad884b85f003b793":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d99a27a34ea94e2ab329c78c9d367b50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc51526763e2424b9b094ade31eb663c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd1cdb04a5af40d794da1e6245e20158":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44d94e813bb24d0e90d97dc89b0eb268","max":224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc51526763e2424b9b094ade31eb663c","value":224}},"e14b66dac939446fb2dd804571872df1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6831516ff3f4a6a8db5dcae856c00be","placeholder":"​","style":"IPY_MODEL_294f13e1ca06483d87569f1c4ed4af45","value":"Downloading builder script: "}},"e4d52d3538f54084a8b1cb6ccd8d7c1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54c5da95f1b46a2bb72dee4e07d0e31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb33b6d155be4c0d81d15576f1af9207":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6831516ff3f4a6a8db5dcae856c00be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"db216568","cell_type":"code","source":"from IPython.display import clear_output\n%pip install datasets==2.19.1\n%pip install tensorflow==2.15.0\n!pip install transformers==4.28.0\n%pip install rouge_score\n%pip install wandb\n%pip install jiwer\n%pip install khmer-nltk\nclear_output()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-04T04:15:14.403378Z","iopub.execute_input":"2025-03-04T04:15:14.403718Z","iopub.status.idle":"2025-03-04T04:15:38.569881Z","shell.execute_reply.started":"2025-03-04T04:15:14.403694Z","shell.execute_reply":"2025-03-04T04:15:38.568934Z"},"papermill":{"duration":106.182297,"end_time":"2024-05-29T13:07:28.693487","exception":false,"start_time":"2024-05-29T13:05:42.511190","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"a920333d","cell_type":"code","source":"\n\nprint(\"----- START IMPORTING LIBRARIES -----\")\n\nimport os\nimport cv2\nimport re\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport rouge_score\nimport datasets\nimport wandb\nimport string\nimport random\nfrom khmernltk import word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\n\nfrom transformers import ViTFeatureExtractor, AutoTokenizer\nfrom transformers import VisionEncoderDecoderModel, XLMRobertaForCausalLM\nfrom transformers import VisionEncoderDecoderConfig\nfrom transformers import DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom transformers import default_data_collator\nfrom transformers import EarlyStoppingCallback\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Log in to Weights & Biases\nwandb.login(key=\"e54bda67bc0e4031b8d6a5b106b28d39585809c4\")\n\n# Define model checkpoints\nprocessor_checkpoint = \"microsoft/trocr-base-printed\"\ntokenizer_checkpoint = \"channudam/khmer-xlm-roberta-base\"\n\n# Load feature extractor and tokenizer\nfeature_extractor = ViTFeatureExtractor.from_pretrained(processor_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n\n# Define a punctuation remover\ntranslator = str.maketrans('', '', string.punctuation)\n\ndef transform_image(image):\n    \"\"\"Apply a random blur to the input image.\"\"\"\n    rand_int = random.randint(1, 2)\n    kernel_size = (rand_int, rand_int)\n    image = cv2.blur(image, kernel_size)\n    return image\nclear_output()\nprint(\"----- LIBRARIES SUCCESSFULLY IMPORTED -----\")\n\n\n\n# print(\"----- STARTING TO IMPORT LIBRARIES -----\")\n\n# import os\n# import cv2\n# import re\n# import torch\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import rouge_score\n# import datasets\n# import wandb\n# import string\n# import random\n# from khmernltk import word_tokenize\n\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data.dataset import Dataset as D\n\n# from transformers import ViTFeatureExtractor, XLMRobertaTokenizer\n# from transformers import VisionEncoderDecoderModel, XLMRobertaForCausalLM\n# from transformers import VisionEncoderDecoderConfig\n# from transformers import DataCollatorForLanguageModeling\n# from transformers import Trainer, TrainingArguments\n# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n# from transformers import default_data_collator\n# from transformers import EarlyStoppingCallback\n# from keras.preprocessing.image import ImageDataGenerator\n\n# # Log in to Weights & Biases\n# wandb.login(key=\"e54bda67bc0e4031b8d6a5b106b28d39585809c4\")\n\n# # Define model checkpoints\n# processor_checkpoint = \"microsoft/trocr-base-handwritten\"\n# tokenizer_checkpoint = \"channudam/khmer-xlm-roberta-base\"\n\n# # Load feature extractor and tokenizer\n# feature_extractor = ViTFeatureExtractor.from_pretrained(processor_checkpoint)\n# tokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_checkpoint)\n\n# # Define a punctuation translator to remove punctuation\n# translator = str.maketrans('', '', string.punctuation)\n\n# def transform_image(image):\n#     \"\"\"\n#     Apply a random blur to the input image.\n#     \"\"\"\n#     rand_int = random.randint(1, 2)\n#     kernel_size = (rand_int, rand_int)\n#     return cv2.blur(image, kernel_size)\n\n# clear_output()\n# print(\"----- LIBRARIES SUCCESSFULLY IMPORTED -----\")","metadata":{"papermill":{"duration":22.486853,"end_time":"2024-05-29T13:07:51.185939","exception":false,"start_time":"2024-05-29T13:07:28.699086","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:16:28.852349Z","iopub.execute_input":"2025-03-04T04:16:28.852799Z","iopub.status.idle":"2025-03-04T04:16:44.256300Z","shell.execute_reply.started":"2025-03-04T04:16:28.852764Z","shell.execute_reply":"2025-03-04T04:16:44.255351Z"}},"outputs":[{"name":"stdout","text":"----- LIBRARIES SUCCESSFULLY IMPORTED -----\n","output_type":"stream"}],"execution_count":3},{"id":"90bc77f5","cell_type":"code","source":"print(\"-----START TO CHECK IF GPU(S) ARE AVAILABLE-----\")\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"papermill":{"duration":0.096271,"end_time":"2024-05-29T13:07:51.287910","exception":false,"start_time":"2024-05-29T13:07:51.191639","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:16:48.218497Z","iopub.execute_input":"2025-03-04T04:16:48.219390Z","iopub.status.idle":"2025-03-04T04:16:48.279817Z","shell.execute_reply.started":"2025-03-04T04:16:48.219347Z","shell.execute_reply":"2025-03-04T04:16:48.278818Z"}},"outputs":[{"name":"stdout","text":"-----START TO CHECK IF GPU(S) ARE AVAILABLE-----\nThere are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":4},{"id":"64baa6e7","cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom khmernltk import word_tokenize\n\n# Read images\nprint(\"-----START TO READ IMAGES-----\")\n\nimage_path = \"/kaggle/input/mptc-2k-mptc-data/MPTC-2K-Image\"\n\n# Ensure directory exists\nif not os.path.exists(image_path):\n    print(f\"Error: Directory does not exist -> {image_path}\")\nelse:\n    # List files and filter PNGs (case-insensitive)\n    files = [file for file in os.listdir(image_path) if file.lower().endswith(\".png\")]\n    \n    # Read images\n    images = np.asarray(\n        [plt.imread(os.path.join(image_path, file)) for file in files],\n        dtype=\"object\"\n    )\n\n    # Extract file names (without extensions) to match with labels\n    label_idx = [os.path.splitext(file)[0] for file in files]\n\n    # Read CSV file\n    labels_path = \"/kaggle/input/mptc-2k-mptc-data/MPTC-2K-Labels.csv\"\n    if not os.path.exists(labels_path):\n        print(f\"Error: CSV file not found -> {labels_path}\")\n    else:\n        labels = pd.read_csv(labels_path)\n\n        # Ensure label indices are valid\n        if \"label\" in labels.columns:\n            labels_dict = labels.set_index(\"filename\")[\"label\"].to_dict()\n            \n            # Retrieve labels for available image files\n            image_names = [labels_dict.get(file, \"\") for file in label_idx]\n\n            # Preprocess labels: Remove tabs, punctuation, and extra spaces\n            image_names = [re.sub(r\"\\\\t\", \" \", name) for name in image_names]\n            image_names = [name.translate(str.maketrans('', '', string.punctuation)) for name in image_names]\n            image_names = [re.sub(r\"\\s+\", \" \", name).strip() for name in image_names]\n            image_names = [\" \".join(word_tokenize(name)) for name in image_names]\n\n            print(\"-----DONE-----\")\n            print(f\"Total number of images read: {len(images)}\")\n        else:\n            print(\"Error: CSV file does not contain a 'label' column.\")\n\n\n# # read image\n# print(\"-----START TO READ IMAGES-----\")\n# image_path = \"/kaggle/input/mptc-2k-mptc-data/MPTC-2K-Image\"\n# files = os.listdir(image_path)\n# images = np.asarray(\n#     [plt.imread(os.path.join(image_path, file)) for file in files if file.endswith(\".jpg\")],\n#     dtype=\"object\")\n# label_idx = [(file.split(\".\")[0]) for file in files if file.endswith(\"jpg\")]\n# labels = pd.read_csv(\"/kaggle/input/mptc-2k-mptc-data/MPTC-2K-Labels.csv\")\n# image_names = [re.sub(r\"\\\\t\", \" \", name) for name in labels.iloc[label_idx]['label'].tolist()]\n# image_names = [image_name.translate(translator) for image_name in image_names]\n# image_names = [re.sub(r\"\\s\", \"\", image_name) for image_name in image_names]\n# image_names = [\" \".join(word_tokenize(image_name)) for image_name in image_names]\n\n# print(\"-----DONE-----\")","metadata":{"papermill":{"duration":2.551542,"end_time":"2024-05-29T13:07:53.845285","exception":false,"start_time":"2024-05-29T13:07:51.293743","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:16:51.551006Z","iopub.execute_input":"2025-03-04T04:16:51.551363Z","iopub.status.idle":"2025-03-04T04:17:09.786398Z","shell.execute_reply.started":"2025-03-04T04:16:51.551334Z","shell.execute_reply":"2025-03-04T04:17:09.785287Z"}},"outputs":[{"name":"stdout","text":"-----START TO READ IMAGES-----\n","output_type":"stream"},{"name":"stderr","text":"| 2025-03-04 04:17:09,728 | \u001b[1;32mINFO\u001b[0m | khmer-nltk | Loaded model from /usr/local/lib/python3.10/dist-packages/khmernltk/word_tokenize/sklearn_crf_ner_10000.sav |\n","output_type":"stream"},{"name":"stdout","text":"-----DONE-----\nTotal number of images read: 2000\n","output_type":"stream"}],"execution_count":5},{"id":"9176dbf0","cell_type":"code","source":"# apply image augmentation\nprint(\"-----START TO AUGMENT IMAGES-----\")\nnew_images = []\nnew_image_names = []\nidx = 0\ndatagen = ImageDataGenerator(\n    brightness_range=[0.4,1], \n    preprocessing_function=transform_image,\n    fill_mode='nearest')\nfor image in images:\n    new_images.append(image)\n    new_image_names.append(image_names[idx])\n    raw_img = np.expand_dims(image, axis=0)\n    aug_iter = datagen.flow(raw_img, batch_size=1)\n    for i in range(1):  #Number of Augment Images\n        augmented_images = next(aug_iter)\n        augmented_bgr_image = augmented_images[0].astype(np.uint8)\n        new_images.append(augmented_bgr_image)\n        new_image_names.append(image_names[idx])\n    idx = idx+1\nnew_images = np.asarray(new_images, dtype=\"object\")\nnew_image_names = np.asarray(new_image_names, dtype=\"object\")\nprint(\"-----DONE-----\")","metadata":{"papermill":{"duration":2.173809,"end_time":"2024-05-29T13:07:56.025283","exception":false,"start_time":"2024-05-29T13:07:53.851474","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:17:15.919427Z","iopub.execute_input":"2025-03-04T04:17:15.919861Z","iopub.status.idle":"2025-03-04T04:17:32.502449Z","shell.execute_reply.started":"2025-03-04T04:17:15.919827Z","shell.execute_reply":"2025-03-04T04:17:32.501544Z"}},"outputs":[{"name":"stdout","text":"-----START TO AUGMENT IMAGES-----\n-----DONE-----\n","output_type":"stream"}],"execution_count":6},{"id":"a5315e2f","cell_type":"code","source":"print(\"-----START TO CONVERT DATA TO PANDAS DATAFRAME-----\")\ndf = pd.DataFrame({\n    'label': new_image_names,\n    'image': [image.flatten() for image in new_images],\n    'shape': [image.shape for image in new_images]\n})\ndf.head()\nprint(\"-----DONE-----\")","metadata":{"papermill":{"duration":0.159675,"end_time":"2024-05-29T13:07:56.191017","exception":false,"start_time":"2024-05-29T13:07:56.031342","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:17:36.892816Z","iopub.execute_input":"2025-03-04T04:17:36.893118Z","iopub.status.idle":"2025-03-04T04:17:43.236356Z","shell.execute_reply.started":"2025-03-04T04:17:36.893095Z","shell.execute_reply":"2025-03-04T04:17:43.235568Z"}},"outputs":[{"name":"stdout","text":"-----START TO CONVERT DATA TO PANDAS DATAFRAME-----\n-----DONE-----\n","output_type":"stream"}],"execution_count":7},{"id":"699a8274","cell_type":"code","source":"print(\"-----START TO SPLIT THE DATASET TO TRAIN TEST-----\")\nX_train, X_test, y_train, y_test = train_test_split(\ndf[['image', 'shape']], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\ntrain_df = pd.DataFrame({\n    'shape': X_train['shape'],\n    'image': X_train['image'],\n    'label': y_train,\n}).reset_index().drop(\"index\", axis=1)\ntest_df = pd.DataFrame({\n    'shape': X_test['shape'],\n    'image': X_test['image'],\n    'label': y_test\n}).reset_index().drop(\"index\", axis=1)\nprint(f\"train_df shape: {train_df.shape}\")\nprint(f\"test_df shape: {test_df.shape}\")\nprint(\"-----DONE-----\")","metadata":{"papermill":{"duration":0.035834,"end_time":"2024-05-29T13:07:56.233017","exception":false,"start_time":"2024-05-29T13:07:56.197183","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:17:45.285077Z","iopub.execute_input":"2025-03-04T04:17:45.285404Z","iopub.status.idle":"2025-03-04T04:17:45.301990Z","shell.execute_reply.started":"2025-03-04T04:17:45.285379Z","shell.execute_reply":"2025-03-04T04:17:45.301251Z"}},"outputs":[{"name":"stdout","text":"-----START TO SPLIT THE DATASET TO TRAIN TEST-----\ntrain_df shape: (3200, 3)\ntest_df shape: (800, 3)\n-----DONE-----\n","output_type":"stream"}],"execution_count":8},{"id":"63cd126f","cell_type":"code","source":"print(\"-----START TO INITIALIZE THE MODEL-----\")\n# set encoder decoder tying to True\nconfig_encoder = VisionEncoderDecoderConfig.from_pretrained(processor_checkpoint, force_download=True).encoder\nconfig_decoder = XLMRobertaForCausalLM.from_pretrained(tokenizer_checkpoint, is_decoder=True, force_download=True).config\nconfig = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder, tie_encoder_decoder=True)\nmodel = VisionEncoderDecoderModel(config)\nmodel.to(device)","metadata":{"papermill":{"duration":20.523851,"end_time":"2024-05-29T13:08:16.762851","exception":false,"start_time":"2024-05-29T13:07:56.239000","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:17:48.759247Z","iopub.execute_input":"2025-03-04T04:17:48.759567Z","iopub.status.idle":"2025-03-04T04:18:18.738699Z","shell.execute_reply.started":"2025-03-04T04:17:48.759546Z","shell.execute_reply":"2025-03-04T04:18:18.737898Z"}},"outputs":[{"name":"stdout","text":"-----START TO INITIALIZE THE MODEL-----\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"549b5586208843b6981e73ab80c7b48e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b533403b016f45138f0ed4a9de036f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9da22e34b8b4f658f6aae252a538711"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): XLMRobertaForCausalLM(\n    (roberta): XLMRobertaModel(\n      (embeddings): XLMRobertaEmbeddings(\n        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): XLMRobertaEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (crossattention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (lm_head): XLMRobertaLMHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (decoder): Linear(in_features=768, out_features=250002, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":9},{"id":"c5442ef6","cell_type":"code","source":"print(\"-----START TO CONFIG THE MODEL-----\")\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 128\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\nprint(\"-----DONE-----\")\n","metadata":{"papermill":{"duration":0.016123,"end_time":"2024-05-29T13:08:16.785698","exception":false,"start_time":"2024-05-29T13:08:16.769575","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:18:31.356363Z","iopub.execute_input":"2025-03-04T04:18:31.356699Z","iopub.status.idle":"2025-03-04T04:18:31.363096Z","shell.execute_reply.started":"2025-03-04T04:18:31.356675Z","shell.execute_reply":"2025-03-04T04:18:31.362132Z"}},"outputs":[{"name":"stdout","text":"-----START TO CONFIG THE MODEL-----\n-----DONE-----\n","output_type":"stream"}],"execution_count":10},{"id":"f1d786dc","cell_type":"code","source":"print(\"-----START TO CONVERT DATAFRAME TO PYTORCH TENSOR-----\")\n\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nbatch_size = 8\nmax_length = 128\n\nclass KhmerTextImageDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor, decoder_max_length=max_length):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n        self.decoder_max_length = decoder_max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        label = self.df.iloc[idx]['label']  # Ensure correct indexing\n        \n        # Load the image safely\n        image_data = self.df.iloc[idx]['image']\n        \n        # Ensure image is a NumPy array\n        if isinstance(image_data, list):  # Convert nested lists to NumPy arrays\n            image_array = np.array(image_data, dtype=np.uint8)\n        else:\n            image_array = image_data.astype(np.uint8)\n\n        # Convert grayscale images to RGB (H, W, 3)\n        if image_array.ndim == 2:  # If 2D (H, W), convert to 3D (H, W, 3)\n            image = Image.fromarray(image_array).convert(\"RGB\")  # Convert grayscale to RGB\n        else:\n            image = Image.fromarray(image_array)  # Already has channels\n\n        # Ensure the image is always in (H, W, 3) format\n        image = np.array(image, dtype=np.uint8)\n\n        # Extract pixel values\n        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n\n        # Tokenize the label\n        labels = self.tokenizer(\n            label,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.decoder_max_length\n        ).input_ids\n\n        # Ensure PAD tokens are ignored in loss computation\n        labels = torch.tensor([label if label != self.tokenizer.pad_token_id else -100 for label in labels])\n\n        encoding = {\n            \"pixel_values\": pixel_values.squeeze(0),  # Remove batch dimension\n            \"labels\": labels\n        }\n        return encoding\n\n# Create train and evaluation datasets\ntrain_dataset = KhmerTextImageDataset(train_df, tokenizer=tokenizer, feature_extractor=feature_extractor)\neval_dataset = KhmerTextImageDataset(test_df, tokenizer=tokenizer, feature_extractor=feature_extractor)\n\nprint(\"-----DONE-----\")\n\n\n# print(\"-----START TO CONVERT DATAFRAME TO PYTHOCH TENSOR-----\")\n# from torch.utils.data.dataset import Dataset as D\n# batch_size = 8\n# max_length = 128\n# class KhmerTextImageDataset(D):\n#     def __init__(self, df, tokenizer, feature_extractor, decoder_max_length=max_length):\n#         self.df = df\n#         self.tokenizer = tokenizer\n#         self.feature_extractor = feature_extractor\n#         self.decoder_max_length = decoder_max_length\n#     def __len__(self):\n#         return len(self.df)\n#     def __getitem__(self, idx):\n#         label = self.df['label'][idx]\n#         image = self.df['image'][idx].reshape(self.df['shape'][idx])\n#         pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n#         labels = self.tokenizer(label, \n#                                 truncation = True,\n#                                 padding=\"max_length\", \n#                                 max_length=self.decoder_max_length).input_ids\n#         # important: make sure that PAD tokens are ignored by the loss function\n#         labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n#         encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n#         return encoding\n# train_dataset = KhmerTextImageDataset(train_df,\n#                                       tokenizer=tokenizer,\n#                                       feature_extractor= feature_extractor)\n# eval_dataset = KhmerTextImageDataset(test_df,\n#                                      tokenizer=tokenizer,\n#                                      feature_extractor= feature_extractor)\n\n# print(\"-----DONE-----\")","metadata":{"papermill":{"duration":0.019346,"end_time":"2024-05-29T13:08:16.812889","exception":false,"start_time":"2024-05-29T13:08:16.793543","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:18:33.587966Z","iopub.execute_input":"2025-03-04T04:18:33.588311Z","iopub.status.idle":"2025-03-04T04:18:33.599442Z","shell.execute_reply.started":"2025-03-04T04:18:33.588282Z","shell.execute_reply":"2025-03-04T04:18:33.598255Z"}},"outputs":[{"name":"stdout","text":"-----START TO CONVERT DATAFRAME TO PYTORCH TENSOR-----\n-----DONE-----\n","output_type":"stream"}],"execution_count":11},{"id":"2a8c6e4b-251f-49cb-a622-5b3f05528fc4","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9c6d68fd","cell_type":"code","source":"print(\"-----START TO INITIALIZE COMPUT METRIC-----\")\nrouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\ncer_metric = datasets.load_metric(\"cer\", trust_remote_code=True)\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    # all unnecessary tokens are removed\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n    return {\n        \"cer\": cer,\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4)\n    }\nclear_output()","metadata":{"papermill":{"duration":1.237056,"end_time":"2024-05-29T13:08:18.056704","exception":false,"start_time":"2024-05-29T13:08:16.819648","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:18:37.499347Z","iopub.execute_input":"2025-03-04T04:18:37.499649Z","iopub.status.idle":"2025-03-04T04:18:38.906950Z","shell.execute_reply.started":"2025-03-04T04:18:37.499627Z","shell.execute_reply":"2025-03-04T04:18:38.906244Z"}},"outputs":[],"execution_count":12},{"id":"f0a54e3f-ce9f-4a53-8c5a-e6e8002dcef5","cell_type":"code","source":"print(\"-----START TO CONVERT DATAFRAME TO PYTORCH TENSOR-----\")\n\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nbatch_size = 8\nmax_length = 128\n\nclass KhmerTextImageDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor, decoder_max_length=max_length):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n        self.decoder_max_length = decoder_max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        label = self.df.iloc[idx]['label']  # Ensure correct indexing\n        \n        # Load the image safely\n        image_data = self.df.iloc[idx]['image']\n        \n        # Ensure image is a NumPy array\n        if isinstance(image_data, list):  # Convert nested lists to NumPy arrays\n            image_array = np.array(image_data, dtype=np.uint8)\n        else:\n            image_array = np.asarray(image_data, dtype=np.uint8)  # Ensure NumPy format\n\n        # ✅ Convert grayscale images to RGB before conversion to NumPy\n        image_pil = Image.fromarray(image_array).convert(\"RGB\")  # Convert grayscale (H, W) to RGB (H, W, 3)\n\n        # ✅ Convert PIL image to NumPy array to ensure correct shape (H, W, 3)\n        image = np.array(image_pil, dtype=np.uint8)\n\n        # ✅ Ensure image has the correct shape\n        if image.ndim == 2:  # If it's still 2D (H, W), add a channel dimension\n            image = np.expand_dims(image, axis=-1)  # Convert (H, W) -> (H, W, 1)\n            image = np.repeat(image, 3, axis=-1)  # Convert (H, W, 1) -> (H, W, 3)\n\n        # ✅ Pass image inside a list to feature_extractor\n        pixel_values = self.feature_extractor([image], return_tensors=\"pt\").pixel_values\n\n        # Tokenize the label\n        labels = self.tokenizer(\n            label,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.decoder_max_length\n        ).input_ids\n\n        # Ensure PAD tokens are ignored in loss computation\n        labels = torch.tensor([label if label != self.tokenizer.pad_token_id else -100 for label in labels])\n\n        encoding = {\n            \"pixel_values\": pixel_values.squeeze(0),  # Remove batch dimension\n            \"labels\": labels\n        }\n        return encoding\n\n# Create train and evaluation datasets\ntrain_dataset = KhmerTextImageDataset(train_df, tokenizer=tokenizer, feature_extractor=feature_extractor)\neval_dataset = KhmerTextImageDataset(test_df, tokenizer=tokenizer, feature_extractor=feature_extractor)\n\nprint(\"-----DONE-----\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:21:21.940024Z","iopub.execute_input":"2025-03-04T04:21:21.940380Z","iopub.status.idle":"2025-03-04T04:21:21.951085Z","shell.execute_reply.started":"2025-03-04T04:21:21.940351Z","shell.execute_reply":"2025-03-04T04:21:21.950287Z"}},"outputs":[{"name":"stdout","text":"-----START TO CONVERT DATAFRAME TO PYTORCH TENSOR-----\n-----DONE-----\n","output_type":"stream"}],"execution_count":16},{"id":"1e3cb9b1","cell_type":"code","source":"\nprint(\"-----START TO SET UP MODEL FINE-TUNING-----\")\n\nKhmerOCRModel = \"/kaggle/working/KhmerOCRModel-V2\"\ntraining_epochs = 20\n\nif not os.path.exists(KhmerOCRModel):\n    os.makedirs(KhmerOCRModel)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=KhmerOCRModel,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    predict_with_generate=True,\n    fp16=True,\n    evaluation_strategy=\"epoch\",  # FIXED ARGUMENT NAME\n    do_train=True,\n    do_eval=True,\n    logging_steps=1024,  \n    save_steps=2048, \n    warmup_steps=1024,  \n    num_train_epochs=training_epochs,\n    overwrite_output_dir=True,\n    save_total_limit=1,\n)\n\ntrainer = Seq2SeqTrainer(\n    tokenizer=feature_extractor,\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=default_data_collator,\n)\n\nprint(\"-----START TO FINE-TUNING-----\")\ntrainer.train()  # START TRAINING\n\n# print(\"-----START TO SET UP MODEL FINE-TUNNING-----\")\n\n# KhmerOCRModel = \"/kaggle/working/KhmerOCRModel\"\n# training_epochs = 20\n\n# if not os.path.exists(KhmerOCRModel):\n#     os.makedirs(KhmerOCRModel)\n\n# training_args = Seq2SeqTrainingArguments(\n#     output_dir=KhmerOCRModel,\n#     per_device_train_batch_size=batch_size,\n#     per_device_eval_batch_size=batch_size,\n#     predict_with_generate=True,\n#     fp16=True,\n#     #eval_strategy=\"epoch\",\n#     evalution_stragtegy=\"epoch\",\n#     do_train=True,\n#     do_eval=True,\n#     logging_steps=1024,  \n#     save_steps=2048, \n#     warmup_steps=1024,  \n#     num_train_epochs = training_epochs,\n#     overwrite_output_dir=True,\n#     save_total_limit=1,\n# )\n\n# trainer = Seq2SeqTrainer(\n#     tokenizer=feature_extractor,\n#     model=model,\n#     args=training_args,\n#     compute_metrics=compute_metrics,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset,\n#     data_collator=default_data_collator,\n# )\n\n# print(\"-----START TO FINE-TUNNING-----\")\n# # Fine-tune the model, training and evaluating on the train dataset trainer.train()\n# trainer.train()","metadata":{"papermill":{"duration":15239.89478,"end_time":"2024-05-29T17:22:17.958570","exception":false,"start_time":"2024-05-29T13:08:18.063790","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T04:21:24.506494Z","iopub.execute_input":"2025-03-04T04:21:24.506813Z"}},"outputs":[{"name":"stdout","text":"-----START TO SET UP MODEL FINE-TUNING-----\n-----START TO FINE-TUNING-----\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='288' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 288/8000 28:20 < 12:44:23, 0.17 it/s, Epoch 0.72/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"id":"a5a4c3ca","cell_type":"code","source":"print(\"-----START TO SAVE THE MODEL-----\")\n# import zipfile\ntrainer.save_model(KhmerOCRModel)\nfeature_extractor.save_pretrained(KhmerOCRModel)\ntokenizer.save_pretrained(KhmerOCRModel)\n# def zipdir(path, ziph):\n#     # ziph is zipfile handle\n#     for root, dirs, files in os.walk(path):\n#         for file in files:\n#             ziph.write(os.path.join(root, file), \n#                        os.path.relpath(os.path.join(root, file), \n#                                        os.path.join(path, '..')))\n# with zipfile.ZipFile('KhmerOCRModel.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n#     zipdir('/kaggle/working/KhmerOCRModel', zipf)","metadata":{"papermill":{"duration":3.372021,"end_time":"2024-05-29T17:22:21.339625","exception":false,"start_time":"2024-05-29T17:22:17.967604","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cc615745","cell_type":"code","source":"from transformers import TrOCRProcessor\nprocessor = TrOCRProcessor.from_pretrained(\"/kaggle/working/KhmerOCRModel-V2\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"/kaggle/working/KhmerOCRModel-V2\")\nclear_output()","metadata":{"papermill":{"duration":14.378681,"end_time":"2024-05-29T17:22:35.727467","exception":false,"start_time":"2024-05-29T17:22:21.348786","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"1aec06ef","cell_type":"code","source":"search_idx = 230\nimage = test_df['image'][search_idx].reshape(test_df['shape'][search_idx])\nprint(test_df['label'][search_idx])\nplt.imshow(image)\nplt.show()","metadata":{"papermill":{"duration":0.229789,"end_time":"2024-05-29T17:22:35.966527","exception":false,"start_time":"2024-05-29T17:22:35.736738","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"7dd3d8c4","cell_type":"code","source":"pixel_values = processor(test_df['image'][search_idx].reshape(test_df['shape'][search_idx]), return_tensors=\"pt\").pixel_values\ngenerated_id = model.generate(pixel_values=pixel_values)\ntext = processor.batch_decode(generated_id, skip_special_tokens=True)\nprint(text)","metadata":{"papermill":{"duration":6.083554,"end_time":"2024-05-29T17:22:42.059708","exception":false,"start_time":"2024-05-29T17:22:35.976154","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"fcbd07b2","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.009439,"end_time":"2024-05-29T17:22:42.079035","exception":false,"start_time":"2024-05-29T17:22:42.069596","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]} 
